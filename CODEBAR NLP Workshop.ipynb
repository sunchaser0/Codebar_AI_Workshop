{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Introduction about myself...\n",
    "#Data Science, NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Poll for definition of Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Key libraries:\n",
    "pandas, nltk?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consider a different model, or parameters for LogReg, or drop all neutrals from the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Corpus: a body of text. 4 is the number of sentences in our corpus; 9 is the number of unique words in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-88c05cb4679f>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-88c05cb4679f>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    {Ask what I could do differently?}\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#exploring the dataset...\n",
    "#cleaning, stopwords\n",
    "#rebalancing\n",
    "# choosing a vectorization:\n",
    "    #TFIDF, Bag of Words, \n",
    "#choose & build a model and train it. And then we're going to evaluate accuracy, and test out other options to see how it changes accuracy.\n",
    "# accuracy\n",
    "#testing different things, \n",
    "{Ask what I could do differently?}\n",
    "{Sample answer: change stopwords removal}\n",
    "#see how that reflects accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## choosing a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remember... this is going into colab so you need to create a colab with a relevant path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('Twitter_Sentiment_Codebar\\\\train.csv')\n",
    "df_2= pd.read_csv('Twitter_Sentiment_Codebar\\\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunchaser\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = df_1.append(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('textID',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So, let's look at our dataset before diving into anything technical. \n",
    "#So, we have a bunch of tweets, each tweet has a unique ID, and it has three features. The selected text, the sentiment of the tweet,\n",
    "# and then the full text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So someone has poured through all this Twitter data, and labelled the sentiment by hand.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       I`d have responded, if I were going\n",
       "1                                  Sooo SAD\n",
       "2                               bullying me\n",
       "3                            leave me alone\n",
       "4                             Sons of ****,\n",
       "                       ...                 \n",
       "3529                                    NaN\n",
       "3530                                    NaN\n",
       "3531                                    NaN\n",
       "3532                                    NaN\n",
       "3533                                    NaN\n",
       "Name: selected_text, Length: 31015, dtype: object"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['selected_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Really useful for companies who want to analyse their social media, answering questions like what do users think aboout our products?\n",
    "#What's interesting here is that if we just look at one example, \"so SAD\" is lablled as negative. Just below you see \"bullying me\", \n",
    "#which is clearly negative. But these two emotions are very different. As an example, if people are sad because they miss something,\n",
    "#it's probably due to a positive sentiment towards that thing. \n",
    "#So, these labels aren't perfect. Real world emotions are subjective and complex. And there are such datasets that deal with \n",
    "#these complexities, so if that's interesting to you, talk to me at the end and I can link you to some of the research & datasets dealing with this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another thing to note, is that if you look, they have selected_text and then the whole text. So, this is kind of a way of filtering out\n",
    "#the less important parts, just really focusing on a key message. So, although selected_text generally might be more representative\n",
    "#of the whole sentiment of the text, it might be good to try out training our model later on the whole text. \n",
    "#Once we have talked through the code & how to use it, why not try it out and see how it changes the model's accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We've had a look at our dataset. We had a think about how the data looks &  the limitations of the dataset\n",
    "#Let's dive into the actually coding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    print(text,type(text))\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        final_word = \"\"\n",
    "        cleaned_word = word.lower()\n",
    "        for char in cleaned_word:\n",
    "            if char in \"abcdefghijklmnopqrstuvwxyz\":\n",
    "                final_word += char\n",
    "        new_text.append(final_word)\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word not in stopwords:\n",
    "            new_text.append(word)\n",
    "    return ' '.join(new_text)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-269-e4ce1e7b8e2f>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-269-e4ce1e7b8e2f>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def rebalance(df):\n",
    "    neutral = 12548\n",
    "    positive = 9685\n",
    "    negative = 8782\n",
    "    \n",
    "#     while [neutral,postitive,negative \n",
    "    \n",
    "    #What does SMOTE, the clever one do again?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'remove'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-292-6f6101a7a10a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbalance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbalance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbalance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'neutral'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m12548\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m8782\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5177\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5178\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5179\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'remove'"
     ]
    }
   ],
   "source": [
    "balance.remove(balance[balance['sentiment']=='neutral'][0:12548-8782])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance = pd.concat([balance, balance[balance['sentiment']=='neutral'][0:12548-8782]]).drop_duplicates(keep=False)\n",
    "balance = pd.concat([balance, balance[balance['sentiment']=='positive'][0:9685-8782]]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>textID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>549e992a42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>088c60f138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>9642c003ef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>358bd9e861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>DANGERously</td>\n",
       "      <td>negative</td>\n",
       "      <td>My Sharpie is running DANGERously low on ink</td>\n",
       "      <td>74a76f6e0a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "      <td>its at 3 am, im very tired but i can`t sleep  ...</td>\n",
       "      <td>e5f0e6ef4b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "      <td>All alone in this old house again.  Thanks for...</td>\n",
       "      <td>416863ce47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "      <td>I know what you mean. My little dog is sinkin...</td>\n",
       "      <td>6332da480c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "      <td>_sutra what is your next youtube video gonna b...</td>\n",
       "      <td>df1baec676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3533</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "      <td>http://twitpic.com/4woj2 - omgssh  ang cute n...</td>\n",
       "      <td>469e15c5a8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26346 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       selected_text sentiment  \\\n",
       "1           Sooo SAD  negative   \n",
       "2        bullying me  negative   \n",
       "3     leave me alone  negative   \n",
       "4      Sons of ****,  negative   \n",
       "12       DANGERously  negative   \n",
       "...              ...       ...   \n",
       "3529             NaN  negative   \n",
       "3530             NaN  positive   \n",
       "3531             NaN  negative   \n",
       "3532             NaN  positive   \n",
       "3533             NaN  positive   \n",
       "\n",
       "                                                   text      textID  \n",
       "1         Sooo SAD I will miss you here in San Diego!!!  549e992a42  \n",
       "2                             my boss is bullying me...  088c60f138  \n",
       "3                        what interview! leave me alone  9642c003ef  \n",
       "4      Sons of ****, why couldn`t they put them on t...  358bd9e861  \n",
       "12         My Sharpie is running DANGERously low on ink  74a76f6e0a  \n",
       "...                                                 ...         ...  \n",
       "3529  its at 3 am, im very tired but i can`t sleep  ...  e5f0e6ef4b  \n",
       "3530  All alone in this old house again.  Thanks for...  416863ce47  \n",
       "3531   I know what you mean. My little dog is sinkin...  6332da480c  \n",
       "3532  _sutra what is your next youtube video gonna b...  df1baec676  \n",
       "3533   http://twitpic.com/4woj2 - omgssh  ang cute n...  469e15c5a8  \n",
       "\n",
       "[26346 rows x 4 columns]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neutral': 12548, 'positive': 9685, 'negative': 8782}\n",
      "neutral 40.45784297920361\n",
      "positive 31.226825729485736\n",
      "negative 28.31533129131066\n"
     ]
    }
   ],
   "source": [
    "sentiment_counts = df['sentiment'].value_counts().to_dict()\n",
    "print(sentiment_counts)\n",
    "for k,v in sentiment_counts.items():\n",
    "    print(k, (v/sum(sentiment_counts.values())*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': 8782, 'neutral': 8782, 'negative': 8782}\n",
      "positive 33.33333333333333\n",
      "neutral 33.33333333333333\n",
      "negative 33.33333333333333\n"
     ]
    }
   ],
   "source": [
    "sentiment_counts = balance['sentiment'].value_counts().to_dict()\n",
    "print(sentiment_counts)\n",
    "for k,v in sentiment_counts.items():\n",
    "    print(k, (v/sum(sentiment_counts.values())*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance = df.copy()\n",
    "balance = balance.groupby('sentiment')\n",
    "balance = pd.DataFrame(balance.apply(lambda x: x.sample(balance.size().min()).reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('negative', 0): 'negative',\n",
       " ('negative', 1): 'negative',\n",
       " ('negative', 2): 'negative',\n",
       " ('negative', 3): 'negative',\n",
       " ('negative', 4): 'negative',\n",
       " ('negative', 5): 'negative',\n",
       " ('negative', 6): 'negative',\n",
       " ('negative', 7): 'negative',\n",
       " ('negative', 8): 'negative',\n",
       " ('negative', 9): 'negative',\n",
       " ('negative', 10): 'negative',\n",
       " ('negative', 11): 'negative',\n",
       " ('negative', 12): 'negative',\n",
       " ('negative', 13): 'negative',\n",
       " ('negative', 14): 'negative',\n",
       " ('negative', 15): 'negative',\n",
       " ('negative', 16): 'negative',\n",
       " ('negative', 17): 'negative',\n",
       " ('negative', 18): 'negative',\n",
       " ('negative', 19): 'negative',\n",
       " ('negative', 20): 'negative',\n",
       " ('negative', 21): 'negative',\n",
       " ('negative', 22): 'negative',\n",
       " ('negative', 23): 'negative',\n",
       " ('negative', 24): 'negative',\n",
       " ('negative', 25): 'negative',\n",
       " ('negative', 26): 'negative',\n",
       " ('negative', 27): 'negative',\n",
       " ('negative', 28): 'negative',\n",
       " ('negative', 29): 'negative',\n",
       " ('negative', 30): 'negative',\n",
       " ('negative', 31): 'negative',\n",
       " ('negative', 32): 'negative',\n",
       " ('negative', 33): 'negative',\n",
       " ('negative', 34): 'negative',\n",
       " ('negative', 35): 'negative',\n",
       " ('negative', 36): 'negative',\n",
       " ('negative', 37): 'negative',\n",
       " ('negative', 38): 'negative',\n",
       " ('negative', 39): 'negative',\n",
       " ('negative', 40): 'negative',\n",
       " ('negative', 41): 'negative',\n",
       " ('negative', 42): 'negative',\n",
       " ('negative', 43): 'negative',\n",
       " ('negative', 44): 'negative',\n",
       " ('negative', 45): 'negative',\n",
       " ('negative', 46): 'negative',\n",
       " ('negative', 47): 'negative',\n",
       " ('negative', 48): 'negative',\n",
       " ('negative', 49): 'negative',\n",
       " ('negative', 50): 'negative',\n",
       " ('negative', 51): 'negative',\n",
       " ('negative', 52): 'negative',\n",
       " ('negative', 53): 'negative',\n",
       " ('negative', 54): 'negative',\n",
       " ('negative', 55): 'negative',\n",
       " ('negative', 56): 'negative',\n",
       " ('negative', 57): 'negative',\n",
       " ('negative', 58): 'negative',\n",
       " ('negative', 59): 'negative',\n",
       " ('negative', 60): 'negative',\n",
       " ('negative', 61): 'negative',\n",
       " ('negative', 62): 'negative',\n",
       " ('negative', 63): 'negative',\n",
       " ('negative', 64): 'negative',\n",
       " ('negative', 65): 'negative',\n",
       " ('negative', 66): 'negative',\n",
       " ('negative', 67): 'negative',\n",
       " ('negative', 68): 'negative',\n",
       " ('negative', 69): 'negative',\n",
       " ('negative', 70): 'negative',\n",
       " ('negative', 71): 'negative',\n",
       " ('negative', 72): 'negative',\n",
       " ('negative', 73): 'negative',\n",
       " ('negative', 74): 'negative',\n",
       " ('negative', 75): 'negative',\n",
       " ('negative', 76): 'negative',\n",
       " ('negative', 77): 'negative',\n",
       " ('negative', 78): 'negative',\n",
       " ('negative', 79): 'negative',\n",
       " ('negative', 80): 'negative',\n",
       " ('negative', 81): 'negative',\n",
       " ('negative', 82): 'negative',\n",
       " ('negative', 83): 'negative',\n",
       " ('negative', 84): 'negative',\n",
       " ('negative', 85): 'negative',\n",
       " ('negative', 86): 'negative',\n",
       " ('negative', 87): 'negative',\n",
       " ('negative', 88): 'negative',\n",
       " ('negative', 89): 'negative',\n",
       " ('negative', 90): 'negative',\n",
       " ('negative', 91): 'negative',\n",
       " ('negative', 92): 'negative',\n",
       " ('negative', 93): 'negative',\n",
       " ('negative', 94): 'negative',\n",
       " ('negative', 95): 'negative',\n",
       " ('negative', 96): 'negative',\n",
       " ('negative', 97): 'negative',\n",
       " ('negative', 98): 'negative',\n",
       " ('negative', 99): 'negative',\n",
       " ('negative', 100): 'negative',\n",
       " ('negative', 101): 'negative',\n",
       " ('negative', 102): 'negative',\n",
       " ('negative', 103): 'negative',\n",
       " ('negative', 104): 'negative',\n",
       " ('negative', 105): 'negative',\n",
       " ('negative', 106): 'negative',\n",
       " ('negative', 107): 'negative',\n",
       " ('negative', 108): 'negative',\n",
       " ('negative', 109): 'negative',\n",
       " ('negative', 110): 'negative',\n",
       " ('negative', 111): 'negative',\n",
       " ('negative', 112): 'negative',\n",
       " ('negative', 113): 'negative',\n",
       " ('negative', 114): 'negative',\n",
       " ('negative', 115): 'negative',\n",
       " ('negative', 116): 'negative',\n",
       " ('negative', 117): 'negative',\n",
       " ('negative', 118): 'negative',\n",
       " ('negative', 119): 'negative',\n",
       " ('negative', 120): 'negative',\n",
       " ('negative', 121): 'negative',\n",
       " ('negative', 122): 'negative',\n",
       " ('negative', 123): 'negative',\n",
       " ('negative', 124): 'negative',\n",
       " ('negative', 125): 'negative',\n",
       " ('negative', 126): 'negative',\n",
       " ('negative', 127): 'negative',\n",
       " ('negative', 128): 'negative',\n",
       " ('negative', 129): 'negative',\n",
       " ('negative', 130): 'negative',\n",
       " ('negative', 131): 'negative',\n",
       " ('negative', 132): 'negative',\n",
       " ('negative', 133): 'negative',\n",
       " ('negative', 134): 'negative',\n",
       " ('negative', 135): 'negative',\n",
       " ('negative', 136): 'negative',\n",
       " ('negative', 137): 'negative',\n",
       " ('negative', 138): 'negative',\n",
       " ('negative', 139): 'negative',\n",
       " ('negative', 140): 'negative',\n",
       " ('negative', 141): 'negative',\n",
       " ('negative', 142): 'negative',\n",
       " ('negative', 143): 'negative',\n",
       " ('negative', 144): 'negative',\n",
       " ('negative', 145): 'negative',\n",
       " ('negative', 146): 'negative',\n",
       " ('negative', 147): 'negative',\n",
       " ('negative', 148): 'negative',\n",
       " ('negative', 149): 'negative',\n",
       " ('negative', 150): 'negative',\n",
       " ('negative', 151): 'negative',\n",
       " ('negative', 152): 'negative',\n",
       " ('negative', 153): 'negative',\n",
       " ('negative', 154): 'negative',\n",
       " ('negative', 155): 'negative',\n",
       " ('negative', 156): 'negative',\n",
       " ('negative', 157): 'negative',\n",
       " ('negative', 158): 'negative',\n",
       " ('negative', 159): 'negative',\n",
       " ('negative', 160): 'negative',\n",
       " ('negative', 161): 'negative',\n",
       " ('negative', 162): 'negative',\n",
       " ('negative', 163): 'negative',\n",
       " ('negative', 164): 'negative',\n",
       " ('negative', 165): 'negative',\n",
       " ('negative', 166): 'negative',\n",
       " ('negative', 167): 'negative',\n",
       " ('negative', 168): 'negative',\n",
       " ('negative', 169): 'negative',\n",
       " ('negative', 170): 'negative',\n",
       " ('negative', 171): 'negative',\n",
       " ('negative', 172): 'negative',\n",
       " ('negative', 173): 'negative',\n",
       " ('negative', 174): 'negative',\n",
       " ('negative', 175): 'negative',\n",
       " ('negative', 176): 'negative',\n",
       " ('negative', 177): 'negative',\n",
       " ('negative', 178): 'negative',\n",
       " ('negative', 179): 'negative',\n",
       " ('negative', 180): 'negative',\n",
       " ('negative', 181): 'negative',\n",
       " ('negative', 182): 'negative',\n",
       " ('negative', 183): 'negative',\n",
       " ('negative', 184): 'negative',\n",
       " ('negative', 185): 'negative',\n",
       " ('negative', 186): 'negative',\n",
       " ('negative', 187): 'negative',\n",
       " ('negative', 188): 'negative',\n",
       " ('negative', 189): 'negative',\n",
       " ('negative', 190): 'negative',\n",
       " ('negative', 191): 'negative',\n",
       " ('negative', 192): 'negative',\n",
       " ('negative', 193): 'negative',\n",
       " ('negative', 194): 'negative',\n",
       " ('negative', 195): 'negative',\n",
       " ('negative', 196): 'negative',\n",
       " ('negative', 197): 'negative',\n",
       " ('negative', 198): 'negative',\n",
       " ('negative', 199): 'negative',\n",
       " ('negative', 200): 'negative',\n",
       " ('negative', 201): 'negative',\n",
       " ('negative', 202): 'negative',\n",
       " ('negative', 203): 'negative',\n",
       " ('negative', 204): 'negative',\n",
       " ('negative', 205): 'negative',\n",
       " ('negative', 206): 'negative',\n",
       " ('negative', 207): 'negative',\n",
       " ('negative', 208): 'negative',\n",
       " ('negative', 209): 'negative',\n",
       " ('negative', 210): 'negative',\n",
       " ('negative', 211): 'negative',\n",
       " ('negative', 212): 'negative',\n",
       " ('negative', 213): 'negative',\n",
       " ('negative', 214): 'negative',\n",
       " ('negative', 215): 'negative',\n",
       " ('negative', 216): 'negative',\n",
       " ('negative', 217): 'negative',\n",
       " ('negative', 218): 'negative',\n",
       " ('negative', 219): 'negative',\n",
       " ('negative', 220): 'negative',\n",
       " ('negative', 221): 'negative',\n",
       " ('negative', 222): 'negative',\n",
       " ('negative', 223): 'negative',\n",
       " ('negative', 224): 'negative',\n",
       " ('negative', 225): 'negative',\n",
       " ('negative', 226): 'negative',\n",
       " ('negative', 227): 'negative',\n",
       " ('negative', 228): 'negative',\n",
       " ('negative', 229): 'negative',\n",
       " ('negative', 230): 'negative',\n",
       " ('negative', 231): 'negative',\n",
       " ('negative', 232): 'negative',\n",
       " ('negative', 233): 'negative',\n",
       " ('negative', 234): 'negative',\n",
       " ('negative', 235): 'negative',\n",
       " ('negative', 236): 'negative',\n",
       " ('negative', 237): 'negative',\n",
       " ('negative', 238): 'negative',\n",
       " ('negative', 239): 'negative',\n",
       " ('negative', 240): 'negative',\n",
       " ('negative', 241): 'negative',\n",
       " ('negative', 242): 'negative',\n",
       " ('negative', 243): 'negative',\n",
       " ('negative', 244): 'negative',\n",
       " ('negative', 245): 'negative',\n",
       " ('negative', 246): 'negative',\n",
       " ('negative', 247): 'negative',\n",
       " ('negative', 248): 'negative',\n",
       " ('negative', 249): 'negative',\n",
       " ('negative', 250): 'negative',\n",
       " ('negative', 251): 'negative',\n",
       " ('negative', 252): 'negative',\n",
       " ('negative', 253): 'negative',\n",
       " ('negative', 254): 'negative',\n",
       " ('negative', 255): 'negative',\n",
       " ('negative', 256): 'negative',\n",
       " ('negative', 257): 'negative',\n",
       " ('negative', 258): 'negative',\n",
       " ('negative', 259): 'negative',\n",
       " ('negative', 260): 'negative',\n",
       " ('negative', 261): 'negative',\n",
       " ('negative', 262): 'negative',\n",
       " ('negative', 263): 'negative',\n",
       " ('negative', 264): 'negative',\n",
       " ('negative', 265): 'negative',\n",
       " ('negative', 266): 'negative',\n",
       " ('negative', 267): 'negative',\n",
       " ('negative', 268): 'negative',\n",
       " ('negative', 269): 'negative',\n",
       " ('negative', 270): 'negative',\n",
       " ('negative', 271): 'negative',\n",
       " ('negative', 272): 'negative',\n",
       " ('negative', 273): 'negative',\n",
       " ('negative', 274): 'negative',\n",
       " ('negative', 275): 'negative',\n",
       " ('negative', 276): 'negative',\n",
       " ('negative', 277): 'negative',\n",
       " ('negative', 278): 'negative',\n",
       " ('negative', 279): 'negative',\n",
       " ('negative', 280): 'negative',\n",
       " ('negative', 281): 'negative',\n",
       " ('negative', 282): 'negative',\n",
       " ('negative', 283): 'negative',\n",
       " ('negative', 284): 'negative',\n",
       " ('negative', 285): 'negative',\n",
       " ('negative', 286): 'negative',\n",
       " ('negative', 287): 'negative',\n",
       " ('negative', 288): 'negative',\n",
       " ('negative', 289): 'negative',\n",
       " ('negative', 290): 'negative',\n",
       " ('negative', 291): 'negative',\n",
       " ('negative', 292): 'negative',\n",
       " ('negative', 293): 'negative',\n",
       " ('negative', 294): 'negative',\n",
       " ('negative', 295): 'negative',\n",
       " ('negative', 296): 'negative',\n",
       " ('negative', 297): 'negative',\n",
       " ('negative', 298): 'negative',\n",
       " ('negative', 299): 'negative',\n",
       " ('negative', 300): 'negative',\n",
       " ('negative', 301): 'negative',\n",
       " ('negative', 302): 'negative',\n",
       " ('negative', 303): 'negative',\n",
       " ('negative', 304): 'negative',\n",
       " ('negative', 305): 'negative',\n",
       " ('negative', 306): 'negative',\n",
       " ('negative', 307): 'negative',\n",
       " ('negative', 308): 'negative',\n",
       " ('negative', 309): 'negative',\n",
       " ('negative', 310): 'negative',\n",
       " ('negative', 311): 'negative',\n",
       " ('negative', 312): 'negative',\n",
       " ('negative', 313): 'negative',\n",
       " ('negative', 314): 'negative',\n",
       " ('negative', 315): 'negative',\n",
       " ('negative', 316): 'negative',\n",
       " ('negative', 317): 'negative',\n",
       " ('negative', 318): 'negative',\n",
       " ('negative', 319): 'negative',\n",
       " ('negative', 320): 'negative',\n",
       " ('negative', 321): 'negative',\n",
       " ('negative', 322): 'negative',\n",
       " ('negative', 323): 'negative',\n",
       " ('negative', 324): 'negative',\n",
       " ('negative', 325): 'negative',\n",
       " ('negative', 326): 'negative',\n",
       " ('negative', 327): 'negative',\n",
       " ('negative', 328): 'negative',\n",
       " ('negative', 329): 'negative',\n",
       " ('negative', 330): 'negative',\n",
       " ('negative', 331): 'negative',\n",
       " ('negative', 332): 'negative',\n",
       " ('negative', 333): 'negative',\n",
       " ('negative', 334): 'negative',\n",
       " ('negative', 335): 'negative',\n",
       " ('negative', 336): 'negative',\n",
       " ('negative', 337): 'negative',\n",
       " ('negative', 338): 'negative',\n",
       " ('negative', 339): 'negative',\n",
       " ('negative', 340): 'negative',\n",
       " ('negative', 341): 'negative',\n",
       " ('negative', 342): 'negative',\n",
       " ('negative', 343): 'negative',\n",
       " ('negative', 344): 'negative',\n",
       " ('negative', 345): 'negative',\n",
       " ('negative', 346): 'negative',\n",
       " ('negative', 347): 'negative',\n",
       " ('negative', 348): 'negative',\n",
       " ('negative', 349): 'negative',\n",
       " ('negative', 350): 'negative',\n",
       " ('negative', 351): 'negative',\n",
       " ('negative', 352): 'negative',\n",
       " ('negative', 353): 'negative',\n",
       " ('negative', 354): 'negative',\n",
       " ('negative', 355): 'negative',\n",
       " ('negative', 356): 'negative',\n",
       " ('negative', 357): 'negative',\n",
       " ('negative', 358): 'negative',\n",
       " ('negative', 359): 'negative',\n",
       " ('negative', 360): 'negative',\n",
       " ('negative', 361): 'negative',\n",
       " ('negative', 362): 'negative',\n",
       " ('negative', 363): 'negative',\n",
       " ('negative', 364): 'negative',\n",
       " ('negative', 365): 'negative',\n",
       " ('negative', 366): 'negative',\n",
       " ('negative', 367): 'negative',\n",
       " ('negative', 368): 'negative',\n",
       " ('negative', 369): 'negative',\n",
       " ('negative', 370): 'negative',\n",
       " ('negative', 371): 'negative',\n",
       " ('negative', 372): 'negative',\n",
       " ('negative', 373): 'negative',\n",
       " ('negative', 374): 'negative',\n",
       " ('negative', 375): 'negative',\n",
       " ('negative', 376): 'negative',\n",
       " ('negative', 377): 'negative',\n",
       " ('negative', 378): 'negative',\n",
       " ('negative', 379): 'negative',\n",
       " ('negative', 380): 'negative',\n",
       " ('negative', 381): 'negative',\n",
       " ('negative', 382): 'negative',\n",
       " ('negative', 383): 'negative',\n",
       " ('negative', 384): 'negative',\n",
       " ('negative', 385): 'negative',\n",
       " ('negative', 386): 'negative',\n",
       " ('negative', 387): 'negative',\n",
       " ('negative', 388): 'negative',\n",
       " ('negative', 389): 'negative',\n",
       " ('negative', 390): 'negative',\n",
       " ('negative', 391): 'negative',\n",
       " ('negative', 392): 'negative',\n",
       " ('negative', 393): 'negative',\n",
       " ('negative', 394): 'negative',\n",
       " ('negative', 395): 'negative',\n",
       " ('negative', 396): 'negative',\n",
       " ('negative', 397): 'negative',\n",
       " ('negative', 398): 'negative',\n",
       " ('negative', 399): 'negative',\n",
       " ('negative', 400): 'negative',\n",
       " ('negative', 401): 'negative',\n",
       " ('negative', 402): 'negative',\n",
       " ('negative', 403): 'negative',\n",
       " ('negative', 404): 'negative',\n",
       " ('negative', 405): 'negative',\n",
       " ('negative', 406): 'negative',\n",
       " ('negative', 407): 'negative',\n",
       " ('negative', 408): 'negative',\n",
       " ('negative', 409): 'negative',\n",
       " ('negative', 410): 'negative',\n",
       " ('negative', 411): 'negative',\n",
       " ('negative', 412): 'negative',\n",
       " ('negative', 413): 'negative',\n",
       " ('negative', 414): 'negative',\n",
       " ('negative', 415): 'negative',\n",
       " ('negative', 416): 'negative',\n",
       " ('negative', 417): 'negative',\n",
       " ('negative', 418): 'negative',\n",
       " ('negative', 419): 'negative',\n",
       " ('negative', 420): 'negative',\n",
       " ('negative', 421): 'negative',\n",
       " ('negative', 422): 'negative',\n",
       " ('negative', 423): 'negative',\n",
       " ('negative', 424): 'negative',\n",
       " ('negative', 425): 'negative',\n",
       " ('negative', 426): 'negative',\n",
       " ('negative', 427): 'negative',\n",
       " ('negative', 428): 'negative',\n",
       " ('negative', 429): 'negative',\n",
       " ('negative', 430): 'negative',\n",
       " ('negative', 431): 'negative',\n",
       " ('negative', 432): 'negative',\n",
       " ('negative', 433): 'negative',\n",
       " ('negative', 434): 'negative',\n",
       " ('negative', 435): 'negative',\n",
       " ('negative', 436): 'negative',\n",
       " ('negative', 437): 'negative',\n",
       " ('negative', 438): 'negative',\n",
       " ('negative', 439): 'negative',\n",
       " ('negative', 440): 'negative',\n",
       " ('negative', 441): 'negative',\n",
       " ('negative', 442): 'negative',\n",
       " ('negative', 443): 'negative',\n",
       " ('negative', 444): 'negative',\n",
       " ('negative', 445): 'negative',\n",
       " ('negative', 446): 'negative',\n",
       " ('negative', 447): 'negative',\n",
       " ('negative', 448): 'negative',\n",
       " ('negative', 449): 'negative',\n",
       " ('negative', 450): 'negative',\n",
       " ('negative', 451): 'negative',\n",
       " ('negative', 452): 'negative',\n",
       " ('negative', 453): 'negative',\n",
       " ('negative', 454): 'negative',\n",
       " ('negative', 455): 'negative',\n",
       " ('negative', 456): 'negative',\n",
       " ('negative', 457): 'negative',\n",
       " ('negative', 458): 'negative',\n",
       " ('negative', 459): 'negative',\n",
       " ('negative', 460): 'negative',\n",
       " ('negative', 461): 'negative',\n",
       " ('negative', 462): 'negative',\n",
       " ('negative', 463): 'negative',\n",
       " ('negative', 464): 'negative',\n",
       " ('negative', 465): 'negative',\n",
       " ('negative', 466): 'negative',\n",
       " ('negative', 467): 'negative',\n",
       " ('negative', 468): 'negative',\n",
       " ('negative', 469): 'negative',\n",
       " ('negative', 470): 'negative',\n",
       " ('negative', 471): 'negative',\n",
       " ('negative', 472): 'negative',\n",
       " ('negative', 473): 'negative',\n",
       " ('negative', 474): 'negative',\n",
       " ('negative', 475): 'negative',\n",
       " ('negative', 476): 'negative',\n",
       " ('negative', 477): 'negative',\n",
       " ('negative', 478): 'negative',\n",
       " ('negative', 479): 'negative',\n",
       " ('negative', 480): 'negative',\n",
       " ('negative', 481): 'negative',\n",
       " ('negative', 482): 'negative',\n",
       " ('negative', 483): 'negative',\n",
       " ('negative', 484): 'negative',\n",
       " ('negative', 485): 'negative',\n",
       " ('negative', 486): 'negative',\n",
       " ('negative', 487): 'negative',\n",
       " ('negative', 488): 'negative',\n",
       " ('negative', 489): 'negative',\n",
       " ('negative', 490): 'negative',\n",
       " ('negative', 491): 'negative',\n",
       " ('negative', 492): 'negative',\n",
       " ('negative', 493): 'negative',\n",
       " ('negative', 494): 'negative',\n",
       " ('negative', 495): 'negative',\n",
       " ('negative', 496): 'negative',\n",
       " ('negative', 497): 'negative',\n",
       " ('negative', 498): 'negative',\n",
       " ('negative', 499): 'negative',\n",
       " ('negative', 500): 'negative',\n",
       " ('negative', 501): 'negative',\n",
       " ('negative', 502): 'negative',\n",
       " ('negative', 503): 'negative',\n",
       " ('negative', 504): 'negative',\n",
       " ('negative', 505): 'negative',\n",
       " ('negative', 506): 'negative',\n",
       " ('negative', 507): 'negative',\n",
       " ('negative', 508): 'negative',\n",
       " ('negative', 509): 'negative',\n",
       " ('negative', 510): 'negative',\n",
       " ('negative', 511): 'negative',\n",
       " ('negative', 512): 'negative',\n",
       " ('negative', 513): 'negative',\n",
       " ('negative', 514): 'negative',\n",
       " ('negative', 515): 'negative',\n",
       " ('negative', 516): 'negative',\n",
       " ('negative', 517): 'negative',\n",
       " ('negative', 518): 'negative',\n",
       " ('negative', 519): 'negative',\n",
       " ('negative', 520): 'negative',\n",
       " ('negative', 521): 'negative',\n",
       " ('negative', 522): 'negative',\n",
       " ('negative', 523): 'negative',\n",
       " ('negative', 524): 'negative',\n",
       " ('negative', 525): 'negative',\n",
       " ('negative', 526): 'negative',\n",
       " ('negative', 527): 'negative',\n",
       " ('negative', 528): 'negative',\n",
       " ('negative', 529): 'negative',\n",
       " ('negative', 530): 'negative',\n",
       " ('negative', 531): 'negative',\n",
       " ('negative', 532): 'negative',\n",
       " ('negative', 533): 'negative',\n",
       " ('negative', 534): 'negative',\n",
       " ('negative', 535): 'negative',\n",
       " ('negative', 536): 'negative',\n",
       " ('negative', 537): 'negative',\n",
       " ('negative', 538): 'negative',\n",
       " ('negative', 539): 'negative',\n",
       " ('negative', 540): 'negative',\n",
       " ('negative', 541): 'negative',\n",
       " ('negative', 542): 'negative',\n",
       " ('negative', 543): 'negative',\n",
       " ('negative', 544): 'negative',\n",
       " ('negative', 545): 'negative',\n",
       " ('negative', 546): 'negative',\n",
       " ('negative', 547): 'negative',\n",
       " ('negative', 548): 'negative',\n",
       " ('negative', 549): 'negative',\n",
       " ('negative', 550): 'negative',\n",
       " ('negative', 551): 'negative',\n",
       " ('negative', 552): 'negative',\n",
       " ('negative', 553): 'negative',\n",
       " ('negative', 554): 'negative',\n",
       " ('negative', 555): 'negative',\n",
       " ('negative', 556): 'negative',\n",
       " ('negative', 557): 'negative',\n",
       " ('negative', 558): 'negative',\n",
       " ('negative', 559): 'negative',\n",
       " ('negative', 560): 'negative',\n",
       " ('negative', 561): 'negative',\n",
       " ('negative', 562): 'negative',\n",
       " ('negative', 563): 'negative',\n",
       " ('negative', 564): 'negative',\n",
       " ('negative', 565): 'negative',\n",
       " ('negative', 566): 'negative',\n",
       " ('negative', 567): 'negative',\n",
       " ('negative', 568): 'negative',\n",
       " ('negative', 569): 'negative',\n",
       " ('negative', 570): 'negative',\n",
       " ('negative', 571): 'negative',\n",
       " ('negative', 572): 'negative',\n",
       " ('negative', 573): 'negative',\n",
       " ('negative', 574): 'negative',\n",
       " ('negative', 575): 'negative',\n",
       " ('negative', 576): 'negative',\n",
       " ('negative', 577): 'negative',\n",
       " ('negative', 578): 'negative',\n",
       " ('negative', 579): 'negative',\n",
       " ('negative', 580): 'negative',\n",
       " ('negative', 581): 'negative',\n",
       " ('negative', 582): 'negative',\n",
       " ('negative', 583): 'negative',\n",
       " ('negative', 584): 'negative',\n",
       " ('negative', 585): 'negative',\n",
       " ('negative', 586): 'negative',\n",
       " ('negative', 587): 'negative',\n",
       " ('negative', 588): 'negative',\n",
       " ('negative', 589): 'negative',\n",
       " ('negative', 590): 'negative',\n",
       " ('negative', 591): 'negative',\n",
       " ('negative', 592): 'negative',\n",
       " ('negative', 593): 'negative',\n",
       " ('negative', 594): 'negative',\n",
       " ('negative', 595): 'negative',\n",
       " ('negative', 596): 'negative',\n",
       " ('negative', 597): 'negative',\n",
       " ('negative', 598): 'negative',\n",
       " ('negative', 599): 'negative',\n",
       " ('negative', 600): 'negative',\n",
       " ('negative', 601): 'negative',\n",
       " ('negative', 602): 'negative',\n",
       " ('negative', 603): 'negative',\n",
       " ('negative', 604): 'negative',\n",
       " ('negative', 605): 'negative',\n",
       " ('negative', 606): 'negative',\n",
       " ('negative', 607): 'negative',\n",
       " ('negative', 608): 'negative',\n",
       " ('negative', 609): 'negative',\n",
       " ('negative', 610): 'negative',\n",
       " ('negative', 611): 'negative',\n",
       " ('negative', 612): 'negative',\n",
       " ('negative', 613): 'negative',\n",
       " ('negative', 614): 'negative',\n",
       " ('negative', 615): 'negative',\n",
       " ('negative', 616): 'negative',\n",
       " ('negative', 617): 'negative',\n",
       " ('negative', 618): 'negative',\n",
       " ('negative', 619): 'negative',\n",
       " ('negative', 620): 'negative',\n",
       " ('negative', 621): 'negative',\n",
       " ('negative', 622): 'negative',\n",
       " ('negative', 623): 'negative',\n",
       " ('negative', 624): 'negative',\n",
       " ('negative', 625): 'negative',\n",
       " ('negative', 626): 'negative',\n",
       " ('negative', 627): 'negative',\n",
       " ('negative', 628): 'negative',\n",
       " ('negative', 629): 'negative',\n",
       " ('negative', 630): 'negative',\n",
       " ('negative', 631): 'negative',\n",
       " ('negative', 632): 'negative',\n",
       " ('negative', 633): 'negative',\n",
       " ('negative', 634): 'negative',\n",
       " ('negative', 635): 'negative',\n",
       " ('negative', 636): 'negative',\n",
       " ('negative', 637): 'negative',\n",
       " ('negative', 638): 'negative',\n",
       " ('negative', 639): 'negative',\n",
       " ('negative', 640): 'negative',\n",
       " ('negative', 641): 'negative',\n",
       " ('negative', 642): 'negative',\n",
       " ('negative', 643): 'negative',\n",
       " ('negative', 644): 'negative',\n",
       " ('negative', 645): 'negative',\n",
       " ('negative', 646): 'negative',\n",
       " ('negative', 647): 'negative',\n",
       " ('negative', 648): 'negative',\n",
       " ('negative', 649): 'negative',\n",
       " ('negative', 650): 'negative',\n",
       " ('negative', 651): 'negative',\n",
       " ('negative', 652): 'negative',\n",
       " ('negative', 653): 'negative',\n",
       " ('negative', 654): 'negative',\n",
       " ('negative', 655): 'negative',\n",
       " ('negative', 656): 'negative',\n",
       " ('negative', 657): 'negative',\n",
       " ('negative', 658): 'negative',\n",
       " ('negative', 659): 'negative',\n",
       " ('negative', 660): 'negative',\n",
       " ('negative', 661): 'negative',\n",
       " ('negative', 662): 'negative',\n",
       " ('negative', 663): 'negative',\n",
       " ('negative', 664): 'negative',\n",
       " ('negative', 665): 'negative',\n",
       " ('negative', 666): 'negative',\n",
       " ('negative', 667): 'negative',\n",
       " ('negative', 668): 'negative',\n",
       " ('negative', 669): 'negative',\n",
       " ('negative', 670): 'negative',\n",
       " ('negative', 671): 'negative',\n",
       " ('negative', 672): 'negative',\n",
       " ('negative', 673): 'negative',\n",
       " ('negative', 674): 'negative',\n",
       " ('negative', 675): 'negative',\n",
       " ('negative', 676): 'negative',\n",
       " ('negative', 677): 'negative',\n",
       " ('negative', 678): 'negative',\n",
       " ('negative', 679): 'negative',\n",
       " ('negative', 680): 'negative',\n",
       " ('negative', 681): 'negative',\n",
       " ('negative', 682): 'negative',\n",
       " ('negative', 683): 'negative',\n",
       " ('negative', 684): 'negative',\n",
       " ('negative', 685): 'negative',\n",
       " ('negative', 686): 'negative',\n",
       " ('negative', 687): 'negative',\n",
       " ('negative', 688): 'negative',\n",
       " ('negative', 689): 'negative',\n",
       " ('negative', 690): 'negative',\n",
       " ('negative', 691): 'negative',\n",
       " ('negative', 692): 'negative',\n",
       " ('negative', 693): 'negative',\n",
       " ('negative', 694): 'negative',\n",
       " ('negative', 695): 'negative',\n",
       " ('negative', 696): 'negative',\n",
       " ('negative', 697): 'negative',\n",
       " ('negative', 698): 'negative',\n",
       " ('negative', 699): 'negative',\n",
       " ('negative', 700): 'negative',\n",
       " ('negative', 701): 'negative',\n",
       " ('negative', 702): 'negative',\n",
       " ('negative', 703): 'negative',\n",
       " ('negative', 704): 'negative',\n",
       " ('negative', 705): 'negative',\n",
       " ('negative', 706): 'negative',\n",
       " ('negative', 707): 'negative',\n",
       " ('negative', 708): 'negative',\n",
       " ('negative', 709): 'negative',\n",
       " ('negative', 710): 'negative',\n",
       " ('negative', 711): 'negative',\n",
       " ('negative', 712): 'negative',\n",
       " ('negative', 713): 'negative',\n",
       " ('negative', 714): 'negative',\n",
       " ('negative', 715): 'negative',\n",
       " ('negative', 716): 'negative',\n",
       " ('negative', 717): 'negative',\n",
       " ('negative', 718): 'negative',\n",
       " ('negative', 719): 'negative',\n",
       " ('negative', 720): 'negative',\n",
       " ('negative', 721): 'negative',\n",
       " ('negative', 722): 'negative',\n",
       " ('negative', 723): 'negative',\n",
       " ('negative', 724): 'negative',\n",
       " ('negative', 725): 'negative',\n",
       " ('negative', 726): 'negative',\n",
       " ('negative', 727): 'negative',\n",
       " ('negative', 728): 'negative',\n",
       " ('negative', 729): 'negative',\n",
       " ('negative', 730): 'negative',\n",
       " ('negative', 731): 'negative',\n",
       " ('negative', 732): 'negative',\n",
       " ('negative', 733): 'negative',\n",
       " ('negative', 734): 'negative',\n",
       " ('negative', 735): 'negative',\n",
       " ('negative', 736): 'negative',\n",
       " ('negative', 737): 'negative',\n",
       " ('negative', 738): 'negative',\n",
       " ('negative', 739): 'negative',\n",
       " ('negative', 740): 'negative',\n",
       " ('negative', 741): 'negative',\n",
       " ('negative', 742): 'negative',\n",
       " ('negative', 743): 'negative',\n",
       " ('negative', 744): 'negative',\n",
       " ('negative', 745): 'negative',\n",
       " ('negative', 746): 'negative',\n",
       " ('negative', 747): 'negative',\n",
       " ('negative', 748): 'negative',\n",
       " ('negative', 749): 'negative',\n",
       " ('negative', 750): 'negative',\n",
       " ('negative', 751): 'negative',\n",
       " ('negative', 752): 'negative',\n",
       " ('negative', 753): 'negative',\n",
       " ('negative', 754): 'negative',\n",
       " ('negative', 755): 'negative',\n",
       " ('negative', 756): 'negative',\n",
       " ('negative', 757): 'negative',\n",
       " ('negative', 758): 'negative',\n",
       " ('negative', 759): 'negative',\n",
       " ('negative', 760): 'negative',\n",
       " ('negative', 761): 'negative',\n",
       " ('negative', 762): 'negative',\n",
       " ('negative', 763): 'negative',\n",
       " ('negative', 764): 'negative',\n",
       " ('negative', 765): 'negative',\n",
       " ('negative', 766): 'negative',\n",
       " ('negative', 767): 'negative',\n",
       " ('negative', 768): 'negative',\n",
       " ('negative', 769): 'negative',\n",
       " ('negative', 770): 'negative',\n",
       " ('negative', 771): 'negative',\n",
       " ('negative', 772): 'negative',\n",
       " ('negative', 773): 'negative',\n",
       " ('negative', 774): 'negative',\n",
       " ('negative', 775): 'negative',\n",
       " ('negative', 776): 'negative',\n",
       " ('negative', 777): 'negative',\n",
       " ('negative', 778): 'negative',\n",
       " ('negative', 779): 'negative',\n",
       " ('negative', 780): 'negative',\n",
       " ('negative', 781): 'negative',\n",
       " ('negative', 782): 'negative',\n",
       " ('negative', 783): 'negative',\n",
       " ('negative', 784): 'negative',\n",
       " ('negative', 785): 'negative',\n",
       " ('negative', 786): 'negative',\n",
       " ('negative', 787): 'negative',\n",
       " ('negative', 788): 'negative',\n",
       " ('negative', 789): 'negative',\n",
       " ('negative', 790): 'negative',\n",
       " ('negative', 791): 'negative',\n",
       " ('negative', 792): 'negative',\n",
       " ('negative', 793): 'negative',\n",
       " ('negative', 794): 'negative',\n",
       " ('negative', 795): 'negative',\n",
       " ('negative', 796): 'negative',\n",
       " ('negative', 797): 'negative',\n",
       " ('negative', 798): 'negative',\n",
       " ('negative', 799): 'negative',\n",
       " ('negative', 800): 'negative',\n",
       " ('negative', 801): 'negative',\n",
       " ('negative', 802): 'negative',\n",
       " ('negative', 803): 'negative',\n",
       " ('negative', 804): 'negative',\n",
       " ('negative', 805): 'negative',\n",
       " ('negative', 806): 'negative',\n",
       " ('negative', 807): 'negative',\n",
       " ('negative', 808): 'negative',\n",
       " ('negative', 809): 'negative',\n",
       " ('negative', 810): 'negative',\n",
       " ('negative', 811): 'negative',\n",
       " ('negative', 812): 'negative',\n",
       " ('negative', 813): 'negative',\n",
       " ('negative', 814): 'negative',\n",
       " ('negative', 815): 'negative',\n",
       " ('negative', 816): 'negative',\n",
       " ('negative', 817): 'negative',\n",
       " ('negative', 818): 'negative',\n",
       " ('negative', 819): 'negative',\n",
       " ('negative', 820): 'negative',\n",
       " ('negative', 821): 'negative',\n",
       " ('negative', 822): 'negative',\n",
       " ('negative', 823): 'negative',\n",
       " ('negative', 824): 'negative',\n",
       " ('negative', 825): 'negative',\n",
       " ('negative', 826): 'negative',\n",
       " ('negative', 827): 'negative',\n",
       " ('negative', 828): 'negative',\n",
       " ('negative', 829): 'negative',\n",
       " ('negative', 830): 'negative',\n",
       " ('negative', 831): 'negative',\n",
       " ('negative', 832): 'negative',\n",
       " ('negative', 833): 'negative',\n",
       " ('negative', 834): 'negative',\n",
       " ('negative', 835): 'negative',\n",
       " ('negative', 836): 'negative',\n",
       " ('negative', 837): 'negative',\n",
       " ('negative', 838): 'negative',\n",
       " ('negative', 839): 'negative',\n",
       " ('negative', 840): 'negative',\n",
       " ('negative', 841): 'negative',\n",
       " ('negative', 842): 'negative',\n",
       " ('negative', 843): 'negative',\n",
       " ('negative', 844): 'negative',\n",
       " ('negative', 845): 'negative',\n",
       " ('negative', 846): 'negative',\n",
       " ('negative', 847): 'negative',\n",
       " ('negative', 848): 'negative',\n",
       " ('negative', 849): 'negative',\n",
       " ('negative', 850): 'negative',\n",
       " ('negative', 851): 'negative',\n",
       " ('negative', 852): 'negative',\n",
       " ('negative', 853): 'negative',\n",
       " ('negative', 854): 'negative',\n",
       " ('negative', 855): 'negative',\n",
       " ('negative', 856): 'negative',\n",
       " ('negative', 857): 'negative',\n",
       " ('negative', 858): 'negative',\n",
       " ('negative', 859): 'negative',\n",
       " ('negative', 860): 'negative',\n",
       " ('negative', 861): 'negative',\n",
       " ('negative', 862): 'negative',\n",
       " ('negative', 863): 'negative',\n",
       " ('negative', 864): 'negative',\n",
       " ('negative', 865): 'negative',\n",
       " ('negative', 866): 'negative',\n",
       " ('negative', 867): 'negative',\n",
       " ('negative', 868): 'negative',\n",
       " ('negative', 869): 'negative',\n",
       " ('negative', 870): 'negative',\n",
       " ('negative', 871): 'negative',\n",
       " ('negative', 872): 'negative',\n",
       " ('negative', 873): 'negative',\n",
       " ('negative', 874): 'negative',\n",
       " ('negative', 875): 'negative',\n",
       " ('negative', 876): 'negative',\n",
       " ('negative', 877): 'negative',\n",
       " ('negative', 878): 'negative',\n",
       " ('negative', 879): 'negative',\n",
       " ('negative', 880): 'negative',\n",
       " ('negative', 881): 'negative',\n",
       " ('negative', 882): 'negative',\n",
       " ('negative', 883): 'negative',\n",
       " ('negative', 884): 'negative',\n",
       " ('negative', 885): 'negative',\n",
       " ('negative', 886): 'negative',\n",
       " ('negative', 887): 'negative',\n",
       " ('negative', 888): 'negative',\n",
       " ('negative', 889): 'negative',\n",
       " ('negative', 890): 'negative',\n",
       " ('negative', 891): 'negative',\n",
       " ('negative', 892): 'negative',\n",
       " ('negative', 893): 'negative',\n",
       " ('negative', 894): 'negative',\n",
       " ('negative', 895): 'negative',\n",
       " ('negative', 896): 'negative',\n",
       " ('negative', 897): 'negative',\n",
       " ('negative', 898): 'negative',\n",
       " ('negative', 899): 'negative',\n",
       " ('negative', 900): 'negative',\n",
       " ('negative', 901): 'negative',\n",
       " ('negative', 902): 'negative',\n",
       " ('negative', 903): 'negative',\n",
       " ('negative', 904): 'negative',\n",
       " ('negative', 905): 'negative',\n",
       " ('negative', 906): 'negative',\n",
       " ('negative', 907): 'negative',\n",
       " ('negative', 908): 'negative',\n",
       " ('negative', 909): 'negative',\n",
       " ('negative', 910): 'negative',\n",
       " ('negative', 911): 'negative',\n",
       " ('negative', 912): 'negative',\n",
       " ('negative', 913): 'negative',\n",
       " ('negative', 914): 'negative',\n",
       " ('negative', 915): 'negative',\n",
       " ('negative', 916): 'negative',\n",
       " ('negative', 917): 'negative',\n",
       " ('negative', 918): 'negative',\n",
       " ('negative', 919): 'negative',\n",
       " ('negative', 920): 'negative',\n",
       " ('negative', 921): 'negative',\n",
       " ('negative', 922): 'negative',\n",
       " ('negative', 923): 'negative',\n",
       " ('negative', 924): 'negative',\n",
       " ('negative', 925): 'negative',\n",
       " ('negative', 926): 'negative',\n",
       " ('negative', 927): 'negative',\n",
       " ('negative', 928): 'negative',\n",
       " ('negative', 929): 'negative',\n",
       " ('negative', 930): 'negative',\n",
       " ('negative', 931): 'negative',\n",
       " ('negative', 932): 'negative',\n",
       " ('negative', 933): 'negative',\n",
       " ('negative', 934): 'negative',\n",
       " ('negative', 935): 'negative',\n",
       " ('negative', 936): 'negative',\n",
       " ('negative', 937): 'negative',\n",
       " ('negative', 938): 'negative',\n",
       " ('negative', 939): 'negative',\n",
       " ('negative', 940): 'negative',\n",
       " ('negative', 941): 'negative',\n",
       " ('negative', 942): 'negative',\n",
       " ('negative', 943): 'negative',\n",
       " ('negative', 944): 'negative',\n",
       " ('negative', 945): 'negative',\n",
       " ('negative', 946): 'negative',\n",
       " ('negative', 947): 'negative',\n",
       " ('negative', 948): 'negative',\n",
       " ('negative', 949): 'negative',\n",
       " ('negative', 950): 'negative',\n",
       " ('negative', 951): 'negative',\n",
       " ('negative', 952): 'negative',\n",
       " ('negative', 953): 'negative',\n",
       " ('negative', 954): 'negative',\n",
       " ('negative', 955): 'negative',\n",
       " ('negative', 956): 'negative',\n",
       " ('negative', 957): 'negative',\n",
       " ('negative', 958): 'negative',\n",
       " ('negative', 959): 'negative',\n",
       " ('negative', 960): 'negative',\n",
       " ('negative', 961): 'negative',\n",
       " ('negative', 962): 'negative',\n",
       " ('negative', 963): 'negative',\n",
       " ('negative', 964): 'negative',\n",
       " ('negative', 965): 'negative',\n",
       " ('negative', 966): 'negative',\n",
       " ('negative', 967): 'negative',\n",
       " ('negative', 968): 'negative',\n",
       " ('negative', 969): 'negative',\n",
       " ('negative', 970): 'negative',\n",
       " ('negative', 971): 'negative',\n",
       " ('negative', 972): 'negative',\n",
       " ('negative', 973): 'negative',\n",
       " ('negative', 974): 'negative',\n",
       " ('negative', 975): 'negative',\n",
       " ('negative', 976): 'negative',\n",
       " ('negative', 977): 'negative',\n",
       " ('negative', 978): 'negative',\n",
       " ('negative', 979): 'negative',\n",
       " ('negative', 980): 'negative',\n",
       " ('negative', 981): 'negative',\n",
       " ('negative', 982): 'negative',\n",
       " ('negative', 983): 'negative',\n",
       " ('negative', 984): 'negative',\n",
       " ('negative', 985): 'negative',\n",
       " ('negative', 986): 'negative',\n",
       " ('negative', 987): 'negative',\n",
       " ('negative', 988): 'negative',\n",
       " ('negative', 989): 'negative',\n",
       " ('negative', 990): 'negative',\n",
       " ('negative', 991): 'negative',\n",
       " ('negative', 992): 'negative',\n",
       " ('negative', 993): 'negative',\n",
       " ('negative', 994): 'negative',\n",
       " ('negative', 995): 'negative',\n",
       " ('negative', 996): 'negative',\n",
       " ('negative', 997): 'negative',\n",
       " ('negative', 998): 'negative',\n",
       " ('negative', 999): 'negative',\n",
       " ...}"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance['sentiment'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `letters` not found.\n",
      "Object `words` not found.\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "Why do we remove punctuation & capital letters?\n",
    "###\n",
    "Why do we remove stop words?\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to actually train a machine learning model with text data, we need to convert it into a format the computer can use.\n",
    "# We need to represent this textual information as numbers, so that the computer can calculate\n",
    "### math of TFIDF\n",
    "#These lists of numbers are called vectors. So what we're doing is vectorizing this text.\n",
    "#There are many ways to use this. The three I'm going to talk to you about today is the Bag of Words, TFIDF and Word2Vec.\n",
    "#So, Bag of Words is the simplest. You basically take every sentence and turn it into a binary list.\n",
    "#So, let's say I have two sentences: The cat is cute and the dog is cute.\n",
    "#using bag of words, you'd turn the cat is cute into this\n",
    "#[1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"the cat is cute the dog is cute\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_c = \"the cat is cute\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_d = \"the dog is cute\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sample.split():\n",
    "    data[word] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data.keys():\n",
    "    if key not in sample_c.split():\n",
    "        data[key].append(0)\n",
    "    else:\n",
    "        data[key].append(1)\n",
    "\n",
    "    if key not in sample_d.split():\n",
    "        data[key].append(0)\n",
    "    else:\n",
    "        data[key].append(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>cat</th>\n",
       "      <th>is</th>\n",
       "      <th>cute</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   the  cat  is  cute  dog\n",
       "0    1    1   1     1    0\n",
       "1    1    0   1     1    1"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "view['sentence'] = pd.DataFrame({\"sentence\":[sample_c,sample_d]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>cat</th>\n",
       "      <th>is</th>\n",
       "      <th>cute</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>the cat is cute</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the dog is cute</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 the  cat  is  cute  dog\n",
       "sentence                                \n",
       "the cat is cute    1    1   1     1    0\n",
       "the dog is cute    1    0   1     1    1"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view.set_index('sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So, this is useful ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "## So basically what this TFIDF does is it gives each word a weight, and this weight says how important "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.tfidf(corpus, clean=True, stopwords=True)>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_1 = \"I really love this painting. No really, I love it!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_2 = \"I really hate this painting. No, really, I hate it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_corpus = [tfidf_1,tfidf_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so, we're going to try code our own TFIDF. It doesn't have to be perfect or work, but we should get an intuition behind what the function actuallly does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(corpus,clean=True,stopwords=True):\n",
    "    \n",
    "    new_corpus = corpus\n",
    "    term_frequency = {}\n",
    "    inverse_document = {}\n",
    "    number_of_sentences = len(corpus)\n",
    "    \n",
    "##Explain clearly the purpose of the stopwords...\n",
    "### do a before and after\n",
    "\n",
    "#BEFORE\n",
    "    print(new_corpus)\n",
    "    \n",
    "    if clean == True:\n",
    "        new_corpus = list([clean_text(doc) for doc in corpus])\n",
    "         \n",
    "    if stopwords == True:\n",
    "        new_corpus = list([remove_stopwords(doc) for doc in new_corpus])\n",
    "    \n",
    "#AFTER\n",
    "    print(new_corpus)\n",
    "    \n",
    "    for doc in new_corpus:\n",
    "        new_doc = doc\n",
    "        for word in new_doc.split():\n",
    "            if word not in term_frequency.keys():\n",
    "                term_frequency[word] = 1\n",
    "            else:\n",
    "                term_frequency[word] +=1 \n",
    "                \n",
    "    for word in term_frequency.keys():\n",
    "        number_of_sentences_word_appears_in = 0\n",
    "        for sentence in new_corpus:\n",
    "            if word in sentence:\n",
    "                number_of_sentences_word_appears_in +=1\n",
    "                \n",
    "        inverse_document[word] = number_of_sentences_word_appears_in\n",
    "        \n",
    "        \n",
    "        \n",
    "    for key in inverse_document.keys():\n",
    "        print(key,log(number_of_sentences/inverse_document[key]))\n",
    "        \n",
    "    \n",
    "    return term_frequency, inverse_document\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(item):\n",
    "    \n",
    "    if item == \"neutral\":\n",
    "        return [1,0,0]\n",
    "    elif item == \"positive\":\n",
    "        return [0,1,0]\n",
    "    if item == \"negative\":\n",
    "        return [0,0,1]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>textID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>cb774db0d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>549e992a42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>088c60f138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>9642c003ef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>358bd9e861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>28b57f3990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>6e0c6d75b1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>50e14c0bb8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>e050245fbd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>fc2cbefa9d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       selected_text sentiment  \\\n",
       "0                I`d have responded, if I were going   neutral   \n",
       "1                                           Sooo SAD  negative   \n",
       "2                                        bullying me  negative   \n",
       "3                                     leave me alone  negative   \n",
       "4                                      Sons of ****,  negative   \n",
       "5  http://www.dothebouncy.com/smf - some shameles...   neutral   \n",
       "6                                                fun  positive   \n",
       "7                                         Soooo high   neutral   \n",
       "8                                        Both of you   neutral   \n",
       "9                       Wow... u just became cooler.  positive   \n",
       "\n",
       "                                                text      textID  \n",
       "0                I`d have responded, if I were going  cb774db0d1  \n",
       "1      Sooo SAD I will miss you here in San Diego!!!  549e992a42  \n",
       "2                          my boss is bullying me...  088c60f138  \n",
       "3                     what interview! leave me alone  9642c003ef  \n",
       "4   Sons of ****, why couldn`t they put them on t...  358bd9e861  \n",
       "5  http://www.dothebouncy.com/smf - some shameles...  28b57f3990  \n",
       "6  2am feedings for the baby are fun when he is a...  6e0c6d75b1  \n",
       "7                                         Soooo high  50e14c0bb8  \n",
       "8                                        Both of you  e050245fbd  \n",
       "9   Journey!? Wow... u just became cooler.  hehe....  fc2cbefa9d  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I really love this painting. No really, I love it!', 'I really hate this painting. No, really, I hate it.']\n",
      "I really love this painting. No really, I love it! <class 'str'>\n",
      "I really hate this painting. No, really, I hate it. <class 'str'>\n",
      "['really love painting really love', 'really hate painting really hate']\n",
      "really 0.0\n",
      "love 0.6931471805599453\n",
      "painting 0.0\n",
      "hate 0.6931471805599453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'really': 4, 'love': 2, 'painting': 2, 'hate': 2},\n",
       " {'really': 2, 'love': 1, 'painting': 2, 'hate': 1})"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf(tfidf_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ask which method seems more suitable for our problem... talk a bit about word2vec/doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Host POLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If we drop all the rows with NaNs, what could a possible problem be? Answers...\n",
    "## Let's look at how many pieces of data we will lose out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df) - len(test_df[test_df.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>textID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fdb77c3752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>f87dea47db</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>96d74cb729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>eee518ae67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>01082688c6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "      <td>its at 3 am, im very tired but i can`t sleep  ...</td>\n",
       "      <td>e5f0e6ef4b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "      <td>All alone in this old house again.  Thanks for...</td>\n",
       "      <td>416863ce47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "      <td>I know what you mean. My little dog is sinkin...</td>\n",
       "      <td>6332da480c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "      <td>_sutra what is your next youtube video gonna b...</td>\n",
       "      <td>df1baec676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3533</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "      <td>http://twitpic.com/4woj2 - omgssh  ang cute n...</td>\n",
       "      <td>469e15c5a8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3535 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     selected_text sentiment  \\\n",
       "314            NaN   neutral   \n",
       "0              NaN   neutral   \n",
       "1              NaN  positive   \n",
       "2              NaN  negative   \n",
       "3              NaN  positive   \n",
       "...            ...       ...   \n",
       "3529           NaN  negative   \n",
       "3530           NaN  positive   \n",
       "3531           NaN  negative   \n",
       "3532           NaN  positive   \n",
       "3533           NaN  positive   \n",
       "\n",
       "                                                   text      textID  \n",
       "314                                                 NaN  fdb77c3752  \n",
       "0     Last session of the day  http://twitpic.com/67ezh  f87dea47db  \n",
       "1      Shanghai is also really exciting (precisely -...  96d74cb729  \n",
       "2     Recession hit Veronique Branquinho, she has to...  eee518ae67  \n",
       "3                                           happy bday!  01082688c6  \n",
       "...                                                 ...         ...  \n",
       "3529  its at 3 am, im very tired but i can`t sleep  ...  e5f0e6ef4b  \n",
       "3530  All alone in this old house again.  Thanks for...  416863ce47  \n",
       "3531   I know what you mean. My little dog is sinkin...  6332da480c  \n",
       "3532  _sutra what is your next youtube video gonna b...  df1baec676  \n",
       "3533   http://twitpic.com/4woj2 - omgssh  ang cute n...  469e15c5a8  \n",
       "\n",
       "[3535 rows x 4 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df['selected_text'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.60228921489602"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((len(test_df) - len(test_df[test_df['selected_text'].isnull()]))/len(test_df)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have a vote... do we use the Text as our main feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, if we did drop All the NANS, there's another problem we might have. If involves something we did earlier to prepare the data...\n",
    "#What is it? Yes, balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['sentiment'] = test_df['sentiment'].apply(one_hot_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Can anyone tell me what this .apply does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = ['This is the first document.','This document is the second document.','And this is the third one.','Is this the first document?']\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>textID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>cb774db0d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>549e992a42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>088c60f138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>9642c003ef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>358bd9e861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         selected_text  sentiment  \\\n",
       "0  I`d have responded, if I were going  [1, 0, 0]   \n",
       "1                             Sooo SAD  [0, 0, 1]   \n",
       "2                          bullying me  [0, 0, 1]   \n",
       "3                       leave me alone  [0, 0, 1]   \n",
       "4                        Sons of ****,  [0, 0, 1]   \n",
       "\n",
       "                                                text      textID  \n",
       "0                I`d have responded, if I were going  cb774db0d1  \n",
       "1      Sooo SAD I will miss you here in San Diego!!!  549e992a42  \n",
       "2                          my boss is bullying me...  088c60f138  \n",
       "3                     what interview! leave me alone  9642c003ef  \n",
       "4   Sons of ****, why couldn`t they put them on t...  358bd9e861  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use invalid data type as a teaching exercise; wrong data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I`d have responded, if I were going <class 'str'>\n",
      "Sooo SAD <class 'str'>\n",
      "bullying me <class 'str'>\n",
      "leave me alone <class 'str'>\n",
      "Sons of ****, <class 'str'>\n",
      "http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth <class 'str'>\n",
      "fun <class 'str'>\n",
      "Soooo high <class 'str'>\n",
      "Both of you <class 'str'>\n",
      "Wow... u just became cooler. <class 'str'>\n",
      "as much as i love to be hopeful, i reckon the chances are minimal =P i`m never gonna get my cake and stuff <class 'str'>\n",
      "like <class 'str'>\n",
      "DANGERously <class 'str'>\n",
      "lost <class 'str'>\n",
      "test test from the LG enV2 <class 'str'>\n",
      "Uh oh, I am sunburned <class 'str'>\n",
      "*sigh* <class 'str'>\n",
      "sick <class 'str'>\n",
      "onna <class 'str'>\n",
      "Hes just not that into you <class 'str'>\n",
      "oh Marly, I`m so sorry!!  I hope you find her soon!! <3 <3 <class 'str'>\n",
      "interesting. <class 'str'>\n",
      "is cleaning the house for her family who is comming later today.. <class 'str'>\n",
      "gotta restart my computer .. I thought Win7 was supposed to put an end to the constant rebootiness <class 'str'>\n",
      "SEe waT I Mean bOuT FoLL0w fRiiDaYs... It`S cALLed LoSe f0LloWeRs FridAy... smH <class 'str'>\n",
      "the free fillin` app on my ipod is fun, im addicted <class 'str'>\n",
      "I`m sorry. <class 'str'>\n",
      ".no internet <class 'str'>\n",
      "fun <class 'str'>\n",
      "Power back up not working too <class 'str'>\n",
      "Quite....heavenly <class 'str'>\n",
      "hope <class 'str'>\n",
      "well so much for being unhappy for about 10 minute <class 'str'>\n",
      "funny. <class 'str'>\n",
      "Ahhh, I slept through the game.  I`m gonna try my best to watch tomorrow though. I hope we play Army. <class 'str'>\n",
      "Thats it, its the end. Tears for Fears <class 'str'>\n",
      "miss <class 'str'>\n",
      "just in case you wonder, we are really busy today and this coming with with adding tons of new blogs and updates stay tuned <class 'str'>\n",
      "soooooo sleeeeepy!!! <class 'str'>\n",
      "A little happy fo <class 'str'>\n",
      "Car not happy, big big dent in boot! Hoping theyre not going to write it off, crossing fingers and waiting <class 'str'>\n",
      "avid fan of <class 'str'>\n",
      "MAYDAY?! <class 'str'>\n",
      "RATT ROCKED NASHVILLE TONITE..ONE THING SUCKED, NO ENCORE!  LIKE IN THE 80`S THEY STILL HAVE A FUN SHOW. PEARCY HAS THAT HOTT BAD BOY LOOK <class 'str'>\n",
      "I love to! <class 'str'>\n",
      "The girl in the hair salon asked me 'Shall I trim your eyebrows!' How old do I feel? <class 'str'>\n",
      "SUCKKKKKK <class 'str'>\n",
      ":visiting my friendster and facebook <class 'str'>\n",
      "dont like go <class 'str'>\n",
      "d I`m not thrilled at all with mine. <class 'str'>\n",
      "Then you should check out http://twittersucks.com and connect with other tweeple who hate twitter <class 'str'>\n",
      "also bored at school, its my third freelesson( freistunde ) <class 'str'>\n",
      "hm... Both of us I guess... <class 'str'>\n",
      "it is ****...u have dissappointed me that past few days <class 'str'>\n",
      "romance zero is funny <class 'str'>\n",
      "I`d rather do the early run..but I am a morning runner <class 'str'>\n",
      "hurts <class 'str'>\n",
      "will be back later. <class 'str'>\n",
      "Torn ace of hearts <class 'str'>\n",
      "what fun are you speaking of? <class 'str'>\n",
      "i lost all my friends, i`m alone and sleepy.. <class 'str'>\n",
      "haha yes <class 'str'>\n",
      "I give in to easily <class 'str'>\n",
      "favorite <class 'str'>\n",
      "jealous.. <class 'str'>\n",
      "Is at a photoshoot. <class 'str'>\n",
      "s awesome <class 'str'>\n",
      "Yay playing a show tonight! Boo it`s gonna soggy and I`m at work right before playing <class 'str'>\n",
      "Chilliin <class 'str'>\n",
      "If you know such agent, do let me know <class 'str'>\n",
      "I still smell of smoke  #kitchenfire <class 'str'>\n",
      "better <class 'str'>\n",
      "Anyone have an extra Keane ticket? I promise to buy you a drink and take rad pics for your FB / Blog / Flickr., etc <class 'str'>\n",
      "'you can ride one, you can catch one, but its not summer til you pop open one'  ? <class 'str'>\n",
      "she is good! so gor-juz yea i kno i asked her yesterday when we were at tha hospital if she talked to u and she said no <class 'str'>\n",
      "OK - I`m out of here for now. Just popped in to say Hi and check on things. I`ll probably head to the guttah later on tonight <class 'str'>\n",
      "BADDD. <class 'str'>\n",
      "My sources say no <class 'str'>\n",
      "I am sooo tired <class 'str'>\n",
      "Hey, you change your twitter account, and you didn`t even tell me... <class 'str'>\n",
      "THANK YYYYYYYYYOOOOOOOOOOUUUUU! <class 'str'>\n",
      "lucky <class 'str'>\n",
      "fell asleep waiting for my ride! <class 'str'>\n",
      "Sick. <class 'str'>\n",
      ", sorry guys <class 'str'>\n",
      "Happy Star Wars day everyone! and Enjoy the holiday (UK) <class 'str'>\n",
      "Miles from you   I`m in Essex so give me plenty of warning so I can arrive in time to get at least one of those free beers <class 'str'>\n",
      "His snoring is so annoying n it keeps me from sleeping (like right now, lol) but I honestly wud miss it if it eva left  I love him. <class 'str'>\n",
      "i miss you bby <class 'str'>\n",
      "cool <class 'str'>\n",
      "Love it there <class 'str'>\n",
      "_Mounce yes and it lasts way past my bedtime! <class 'str'>\n",
      "Hi  how are you doing ???  *just joined twitter... <class 'str'>\n",
      "tired <class 'str'>\n",
      "eating ice cream and then getting ready for graduation. <class 'str'>\n",
      "Happy Mothers day to all you Mums out there <class 'str'>\n",
      "freaked <class 'str'>\n",
      "unfortunately <class 'str'>\n",
      "Gonna read a story bout adam lambert online then bed. Nighty night <class 'str'>\n",
      "best <class 'str'>\n",
      "Pretty <class 'str'>\n",
      "Certainly not Cheers than, huh? <class 'str'>\n",
      "horrible, <class 'str'>\n",
      "busy <class 'str'>\n",
      "Awesome. <class 'str'>\n",
      "at least I get to watch over time  Let`s go Pens!! <class 'str'>\n",
      "cool i wear black most of the time when i go out <class 'str'>\n",
      "thanks <class 'str'>\n",
      "safe <class 'str'>\n",
      "I wish I was allowed to go <class 'str'>\n",
      "if u have a friendster add me!!!!!!!!!        my email adress      add me  loco_crime_1st.com        add me <class 'str'>\n",
      "has tickets.......? <class 'str'>\n",
      "Thank you, <class 'str'>\n",
      "ACSM. it`s unfathomable.  i think the other one .. and the .. is one that should be kept to the comfort of our bedrooms. yes? <class 'str'>\n",
      "I love my <class 'str'>\n",
      "I don`t feel confident <class 'str'>\n",
      "sad. <class 'str'>\n",
      "hahaa your awesomee ! <class 'str'>\n",
      "awesomeeeee <class 'str'>\n",
      "I hate Fallout 3 it keeps making me jump, I`m also low on health, money, ammo and food  don`t worry I`ll get through it. <class 'str'>\n",
      "I had it! On my itunes, but then I lost all my songs. <class 'str'>\n",
      "What`s with the gloomy weather? The sun must be too tired to come out and play  heading to victoria gardens for some impulse buys haha <class 'str'>\n",
      "Not looking forward <class 'str'>\n",
      "Poor you <class 'str'>\n",
      "not well <class 'str'>\n",
      "Not a prob <class 'str'>\n",
      "at dads, watching some mtv and am going on sims2 in a minutee <class 'str'>\n",
      "Absolutely <class 'str'>\n",
      "what`s the matter chickadee? <class 'str'>\n",
      "y adore <class 'str'>\n",
      "good <class 'str'>\n",
      "I love him too <class 'str'>\n",
      "painful. <class 'str'>\n",
      "sad? <class 'str'>\n",
      "e nice <class 'str'>\n",
      "i wish <class 'str'>\n",
      "Namaskar & Namaste r both the same. Marathi people say Namaskar! its a marathi word.... should i ? ...naaaah ! <class 'str'>\n",
      "Congrats!  I cuss like that in a matter of minutes, But didn`t know until now there is a reward for it. <class 'str'>\n",
      "Humous and Dorito`s.... Oh yes <class 'str'>\n",
      "missed all the awesome weather, <class 'str'>\n",
      "Today is going to be a normal day for I hope. We had a group of pilots from a large airline come in last night so it was too much drink <class 'str'>\n",
      "terrible! <class 'str'>\n",
      "Unfortunatley, <class 'str'>\n",
      "That sucks, tho. <class 'str'>\n",
      "Hate fighting <class 'str'>\n",
      "I watched that too!!! I didnt want her to win, but she put up a good fight..lol <class 'str'>\n",
      "Car-warmed Sprite tastes like sore throat <class 'str'>\n",
      "Just came 11th in cross country and beat dumbo <class 'str'>\n",
      "enjoyable. <class 'str'>\n",
      "endearing- <class 'str'>\n",
      "tomorrow valeria`s lunch!!! going to get my hair done but im arraving late   got my cousins babtizm or whatever you spell it <class 'str'>\n",
      "goooooddd morning tweets!! <class 'str'>\n",
      "hate <class 'str'>\n",
      "fine! <class 'str'>\n",
      "i don`t like the other ones. <class 'str'>\n",
      "Mmmmmmmm... ? it in the morning <class 'str'>\n",
      "me neither <class 'str'>\n",
      "Has about 10 hours work to do, on a Sunday. Boo. I will find time for a two hour lunchbreak though. Yeah <class 'str'>\n",
      "Bugger. forgot I still have washing in my machine <class 'str'>\n",
      "sending love, blessings & healing thoughts to you & family  peace <class 'str'>\n",
      ".really bad <class 'str'>\n",
      "ah yes, I know that feeling <class 'str'>\n",
      "Night of the cookers with my dad <class 'str'>\n",
      "brutal <class 'str'>\n",
      "Nope I am in Coquitlam <class 'str'>\n",
      "boring <class 'str'>\n",
      "p sounds like fun <class 'str'>\n",
      "Big booming thunder storm almost here.  Maybe we can all go home early???  Ah... probably not. <class 'str'>\n",
      "great <class 'str'>\n",
      "excited <class 'str'>\n",
      "good morning <class 'str'>\n",
      "its the best show EVER! <class 'str'>\n",
      "messed <class 'str'>\n",
      "i think iv hurt my tooth  and eilish and cassie are having a drawing competiton to draw cookies and pineapples haha :L . <class 'str'>\n",
      "I want to know when the auditions are Mander! Text or...reply please! <class 'str'>\n",
      "NOT THE SECRET NAMEREBECCA PLEASE <class 'str'>\n",
      "miss <class 'str'>\n",
      "i need to get my computer fixed <class 'str'>\n",
      "illness <class 'str'>\n",
      "All the cool people I want to find for following today are #English, and I guess the English don`t tweet. <class 'str'>\n",
      "no sir...i woulda put honey...but i don`t have any <class 'str'>\n",
      "i totally loved it! <class 'str'>\n",
      "i <3 you. u helped me thru the hrdest time of my life! <class 'str'>\n",
      "sad <class 'str'>\n",
      "Finally got a call for marriage counseling 3 days late.... <class 'str'>\n",
      "ok then <class 'str'>\n",
      "why baby? <class 'str'>\n",
      "sick! <class 'str'>\n",
      "We`re having an impromptu pool party... Except I don`t know how to swim so I can`t get in <class 'str'>\n",
      "oww <class 'str'>\n",
      "happy 1 year! <3 <class 'str'>\n",
      "Goooooooooooood Morrrrrrrrning <class 'str'>\n",
      "*phew*  Will make a note in case anyone else runs into the same issueï¿½ <class 'str'>\n",
      "WHAT ABOUT ME ??  I VOTE EVERY DAY FOR YOU !!!!! <class 'str'>\n",
      "This diet is killing me <class 'str'>\n",
      "i talk to you <class 'str'>\n",
      "bored.. <class 'str'>\n",
      "e fun <class 'str'>\n",
      "So far, so good, but I did sleep for most of those 4 hours. Getting a bit twitchy now <class 'str'>\n",
      "What`s with Twatter lately?  Either I can`t get on or the replies don`t turn up! <class 'str'>\n",
      "lost my voice <class 'str'>\n",
      "hate <class 'str'>\n",
      "I`ve heard this fall. I`m waiting too! <class 'str'>\n",
      "more nightmares?  *huggles* <class 'str'>\n",
      "I AM SUCH A CREEPER  I feel disappointed because of it. **** my cyberstalking skills   the internet = no more privacy <class 'str'>\n",
      "headache <class 'str'>\n",
      "happy <class 'str'>\n",
      "Grabbing coffee from  then making mom breakfast <class 'str'>\n",
      "going to have fun <class 'str'>\n",
      "thanks. before the major chop. <class 'str'>\n",
      "d thank you! <class 'str'>\n",
      "just got up and updated my ipod <class 'str'>\n",
      "HAHAHA <class 'str'>\n",
      "crush <class 'str'>\n",
      "Saw James carville in the store today. His head is really that bald <class 'str'>\n",
      "yellow for   ? http://blip.fm/~5z05g <class 'str'>\n",
      "which means you`re just going to have to come back to vancouver and have it our way! hahah <class 'str'>\n",
      "Feeling smooth <class 'str'>\n",
      "Ew traffic <class 'str'>\n",
      "downloading songs while trying to sneak a lil homework in too, which should be my main priority not songs lol <class 'str'>\n",
      "your not alone...i need coffee too. <class 'str'>\n",
      "Sounds like me <class 'str'>\n",
      "Bad day  The day you realize what mess you`ve put me through will be one of the happiest days of my life... <class 'str'>\n",
      "I hate not having a bike.. <class 'str'>\n",
      "_nesmith <class 'str'>\n",
      "This is worse than taxes. <class 'str'>\n",
      "JONAS BROTHERS - Live to party.                It`s rocking so hard <class 'str'>\n",
      "happy <class 'str'>\n",
      "would love to test it though. <class 'str'>\n",
      "Fun night! <class 'str'>\n",
      "The exception for a short dude: Larenz fineass Tate yum <class 'str'>\n",
      "I thought Wolverine was awesome. <class 'str'>\n",
      "Hopefully <class 'str'>\n",
      "Yes i work 6 to 3... <class 'str'>\n",
      "Hmmm, maybe that`s what they meant. They eluded to something brand new but you know how the media is <class 'str'>\n",
      "Done at the spa   now meeting vic for some late lunch! <class 'str'>\n",
      "Happy <class 'str'>\n",
      "Always have wanted to go to Oz <class 'str'>\n",
      "Thx <class 'str'>\n",
      "luv <class 'str'>\n",
      "that`s why I need to be there <class 'str'>\n",
      "Okay so I`m dedicating my 300th tweet to the fact that I`m going to the Apple store because there is a HUGE crack on the glass screen! <class 'str'>\n",
      "If only we could ever actually be allowed to stay here and do that <class 'str'>\n",
      "Let me know how that turns out!! <class 'str'>\n",
      "was that sass I detect?  As long as it isn`t back sass! Haha <class 'str'>\n",
      ",ahaha <class 'str'>\n",
      "Sports Bar Shatranjanpoli Rest Ph 26498457 All Sports Bar Andheri W 26733333 Dont know whether that helps. Google ki jai ho <class 'str'>\n",
      "I`m not sleeping at all un <class 'str'>\n",
      "Seriously, <class 'str'>\n",
      "I live for pain, bring it on <class 'str'>\n",
      "okay, i`m out for a while  back later! <class 'str'>\n",
      "g What is this powerblog challenge you keep talking about?  I`m a newbie followe <class 'str'>\n",
      "died. <class 'str'>\n",
      "Waiting for tish to get off. Got to drive my moms CRV to pick her up all my myself and duckie. First time <class 'str'>\n",
      "not going to well! <class 'str'>\n",
      "going to shower because i don`t want to smell at school tomorrow <class 'str'>\n",
      "Sigh... you know I am... <class 'str'>\n",
      "hopefully <class 'str'>\n",
      "Here are 4 FREE twitter tools will get you followers  http://short.to/511q http://jijr.com/hulz http://short.to/511r http://2ve.org/xPG0/ <class 'str'>\n",
      "just got home from work.... and is chugging down a big bottle of apple juice. <class 'str'>\n",
      "g harmed <class 'str'>\n",
      "what an amazing day. <class 'str'>\n",
      "it was only once for my big brother...and I`m done now <class 'str'>\n",
      "good news: finally finished my #EASactive workout that has been paused for 6 hours. bad news: my resistance band is torn <class 'str'>\n",
      "Well good morning all, What a wonderful day in the neighborhood  Thanks for all those that are now following another 60 this morning <class 'str'>\n",
      "were amazing tonight.. <class 'str'>\n",
      "hope <class 'str'>\n",
      "it hurts too much... <class 'str'>\n",
      "I waited, listening to wind blowing through the tumbleweed? Are none of you old enough to know what to do when someone says 'Crackerack'? <class 'str'>\n",
      "Hell Yeah! <class 'str'>\n",
      "HIM shirt at dinner? Do you need to ask??  Does it actually have Ville on it? <class 'str'>\n",
      "i know!! <class 'str'>\n",
      "huh what the ****? Smelly? Noooo. I love alex Vixon <class 'str'>\n",
      "GREAT <class 'str'>\n",
      "Happy b-day! <class 'str'>\n",
      "thank <class 'str'>\n",
      "HAPPY <class 'str'>\n",
      "glad <class 'str'>\n",
      "Thanks <class 'str'>\n",
      "twittering after 2 days! <class 'str'>\n",
      "Its too nice <class 'str'>\n",
      "depressed <class 'str'>\n",
      "sick. <class 'str'>\n",
      "Happy Mothers Day people. i love my mom a lot still <class 'str'>\n",
      "sorry <class 'str'>\n",
      ", he was soooo friendly. <class 'str'>\n",
      "happy <class 'str'>\n",
      "Is getting the hang of Twitter. <class 'str'>\n",
      "You know your neck is jacked up when you are forced to pay for parking bc you can`t turn you head to parallel park in the free spaces... <class 'str'>\n",
      "I want it BACK NOW!: <class 'str'>\n",
      "miss <class 'str'>\n",
      "bathing with two little angels, Keyla and Janice. <class 'str'>\n",
      "chocked <class 'str'>\n",
      "HAHAHHA lol true that! i always remember my BD but i can never remember what date or even day it is <class 'str'>\n",
      "Family is here,hanging with them <class 'str'>\n",
      ".it`s gorgeous out! <class 'str'>\n",
      "m worried <class 'str'>\n",
      "Im not bannished. <class 'str'>\n",
      "Morning! If I get to see it, I`ll let you know. Right now, I`m going to go see Wolverine. <class 'str'>\n",
      "red top tabloids, build em up, knock em down <class 'str'>\n",
      "tonight in party w/ my girls (minus vita) <class 'str'>\n",
      "why not now you made me sad I thought you`d be jumping for joy <class 'str'>\n",
      "Simple my a#@ <class 'str'>\n",
      "The pics I just uploaded are the baby pics of my cats. Missy is now an adult and a pretty little kitty, but Batty is in kitten heaven now <class 'str'>\n",
      "amazing <class 'str'>\n",
      "glad <class 'str'>\n",
      "I have missed them <class 'str'>\n",
      "_Live That`s what I want. More the better. Bound to be a few bad eggs though, but they will soon learn. <class 'str'>\n",
      "Hell yeah Kellynn got a Twitter. Finally. <class 'str'>\n",
      "as wort <class 'str'>\n",
      "better <class 'str'>\n",
      "Ship. I`m stuck. <class 'str'>\n",
      "Cannot wait <class 'str'>\n",
      "shame <class 'str'>\n",
      "nan <class 'float'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-219-e27631574698>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'selected_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'selected_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4040\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4041\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4042\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4044\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-184-0004ba1b982f>\u001b[0m in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mnew_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mfinal_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mcleaned_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "df['selected_text'] = df['selected_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'inde'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-220-54d7d7217d10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minde\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5177\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5178\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5179\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'inde'"
     ]
    }
   ],
   "source": [
    "df.inde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-222-491c9eae1412>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselected_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1601\u001b[0m         \"\"\"\n\u001b[0;32m   1602\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1603\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1604\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1605\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1032\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    940\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    326\u001b[0m                                                tokenize)\n\u001b[0;32m    327\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 328\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[0;32m    144\u001b[0m                              \"unicode string.\")\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "vectors = vectorizer.fit_transform(test_df.selected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "##So what this code snippet above is a library that does the function we've just looked into. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
